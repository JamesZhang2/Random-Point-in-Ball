\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{import}
\import{.}{preamble.tex}

\title{How to Select a Random Point from a High-Dimensional Ball}
\author{James Zhang}
\date{July 23, 2022}

\begin{document}

\maketitle

\section{Introduction}

Hey everyone, I'm James. Last year, I watched some Summer of Math Exposition videos, and I really enjoyed them, so this year, I'm gonna make one myself.

One of the videos from last year that caught my attention was \href{https://www.youtube.com/watch?v=4y_nmpv-9lI&list=PLnQX-jgAF5pTkwtUuVpqS5tuWmJ-6ZM-Z&index=6&t=3s}{``The Best Way to Find a Random Point in a Circle"} by Justin. Coincidentally, two of the courses that I took at university this year mentioned higher-dimensional volumes and geometry. So in this video, I'm going to extend Justin's results and talk about how to select a random point from a high-dimensional ball.

First of all, let me clarify what I mean by a high-dimensional ball. A ball is a generalization of a circle. A unit circle (and its interior) has equation $x^2 + y^2 = 1$. An $n$-dimensional ball has equation $x_1^2 + \ldots + x_n^2 = 1$.

\section{Rejection Sampling}

The first method that Justin mentioned in his video was rejection sampling. That generalizes easily to $n$ dimensions, so I'm going to implement it. The idea of rejection sampling is that we select a random point from a box, where each coordinate is selected uniformly at random between -1 and 1. If the point selected happened to be in the unit ball, we're done. If not, we reject that point and select another one. Let's see how this goes:

\subsection{Python Simulation}

I selected 3141 points for dimensions 1 to 8 and recorded the average number of trials. Here are the results:

% Results from Jupyter Notebook

% [1.0, 1.9872652021649155, 6.037567653613499, 23.28430436166826, 122.61095192613817, 711.0792741165234, 5022.741483603948, 40197.09328239414]

As you can see, the number of trials seems to grow exponentially with dimension. In 8 dimensions, each point on average requires way over 10,000 trials. We can imagine that in higher dimensions, it takes even more trials. How did that happen?

\subsection{Volume of the Unit Ball}

Let $X$ be the number of trials that we need to select a point in the ball. Since each trial is independent, and we repeat until we succeed, $X$ is a geometric random variable. The probability that we select a point in the ball in any particular trial is equal to the ratio between the $n$-dimensional volume of the ball and the $n$-dimensional volume of the box. The box has side length 2, so its $n$-dimensional volume is $2^n$. What is the $n$-dimensional volume of the ball?

% Reference: Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach by Hubbard & Hubbard

Let $\beta_n$ be the volume of the $n$-dimensional unit ball. We will use integration and Fubini's Theorem to find out $\beta_n$.

We first choose a variable, say $x_1$, and we integrate it from -1 to 1. We cut the ball into small slices, where each slice can be approximated by a small cylinder. The height of the cylinder is $dx_1$, and the base is an $(n-1)$-dimensional ball. In the special case of three dimensions, the base of each slice is a circle. The radius of the base is $r = \sqrt{1 - x_1^2}$ by the Pythagorean Theorem. Since volume scales appropriately according to dimension, the $(n-1)$-dimensional volume of the base is $r^{n-1} \beta_{n-1}$, where $\beta_{n-1}$ is the volume of the $(n-1)$-dimensional unit ball. Thus,
\[
  \beta_n = \int_{-1}^1 r^{n-1} \beta_{n-1} \, dx_1 = \beta_{n-1} \int_{-1}^1 (1 - x_1^2)^{\frac{n-1}{2}} \, dx_1.
\]

We pulled out $\beta_{n-1}$ from the integral because it is just a constant. It remains to figure out the integral
\[
  c_n = \int_{-1}^1 (1 - t^2)^{\frac{n-1}{2}} \, dt
\]
because then we have the recurrence relation $\beta_n = c_n \beta_{n - 1}$.

Trig substitution with $t = \sin(\theta)$ gives
\begin{align*}
  c_n     & = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \cos^{n}(\theta) \, d\theta   \\
  c_{n-2} & = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \cos^{n-2}(\theta) \, d\theta
\end{align*}

Integration by parts for $c_n$, with $u = \cos^{n-1}(\theta)$ and $dv = \cos(\theta) \, d\theta$ gives

\[
  c_n = \cos^{n-1}(\theta) \sin(\theta) \bigg|_{-\frac{\pi}{2}}^{\frac{\pi}{2}} + \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \sin^2(\theta) (n-1) \cos^{n-2}(\theta) \, d\theta
\]

The first term on the right hand side is 0, so
\[
  c_n = (n-1) \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \sin^2(\theta) \cos^{n-2}(\theta) \, d\theta
\]

Recall that $\sin^2(\theta) = 1 - \cos^2(\theta)$, so
\[
  c_n = (n-1) \bracks{\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \cos^{n-2}(\theta) \, d\theta - \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \cos^n(\theta) \, d\theta} = (n-1) c_{n-2} - (n-1) c_n
\]

Rearranging, we get
\[
  c_n = \frac{n-1}{n}c_{n-2}.
\]

The base cases are $c_1 = 2$ (by simple integration) and $c_2 = \frac{\pi}{2}$ (area of a semicircle). From geometry, we have $\beta_1 = 2$, $\beta_2 = \pi$, $\beta_3 = \frac{4}{3}\pi$.

Recall that $\beta_n = c_n \beta_{n - 1}$. In other words, the ratio between $\beta_n$ and $\beta_{n - 1}$ is $c_n$. If $c_n$ were a constant that is less than 2, say 1.5, then every time we raise a dimension, we double the volume of the box, but the volume of the ball only increases by a factor of 1.5. The ratio between the volume of the ball and the volume of the box will thus decrease by a factor of $\frac{3}{4}$. Since the probability that a particular trial succeeds is equal to that ratio, the probability will decay exponentially by a factor of $\frac{3}{4}$ each time.

However, in reality, $c_n = \frac{n-1}{n}c_{n-2}$, and since $\frac{n-1}{n} < 1$, this means that the multiplier $c_n$ is not a constant: it decreases every time! In fact, there will be an $n$ such that $c_n < 1$, and from that point on, the volume of the ball decreases as we increase dimension. (Here, we're talking about the numerical value of the volume, ignoring the units.)

Therefore, the ratio and thus the probability decays faster than exponentially. Since for a geometric random variable $X$ with success probability $p$, the expected value of $X$ is $\frac{1}{p}$, this implies that the expected number of trials needed to sample a valid point increases faster than exponentially.

\section{Sphere and Distance Method}

Another method to select a random point from a circle that Justin mentioned was to turn to polar coordinates. We first choose $\theta$ uniformly, then choose $r$ so that the points $(r, \theta)$ are distributed uniformly in the circle.

We can try to apply this idea to higher dimensions. In three dimensions, we can try to choose $r, \theta, \vphi$ in some way such that the points represented by $(r, \theta, \vphi)$ in spherical coordinates are distributed uniformly in a three-dimensional ball. However, we can't choose $\vphi$ uniformly because when $\vphi$ is close to $\frac{\pi}{2}$, that is, we're choosing points near the equator, we expect to see way more points than when $\vphi$ is close to 0 or $\pi$ (or near the poles). There are higher-dimensional analogs of the spherical coordinates in higher dimensions, but we can imagine that as the dimensions go up, it would be increasingly difficult to keep track of the distributions of each variable.

Instead, we take a similar approach: First choose a direction uniformly at random, and then choose a number $r$ between 0 and 1. The random point will be in that direction with a distance $r$ from the origin.

Choosing a random direction in n-dimensional space is the same as choosing a random point on the n-dimensional unit sphere. A sphere is the boundary of a ball, and points on the unit sphere satisfy $x_1^2 + \ldots + x_n^2 = 1$.

So now, our task is divided into two parts: Choosing a random point on the unit sphere and choosing a distance from the origin. We'll focus on the first part for now and worry about choosing a distance later.

\subsection{Random Point on Sphere}

\subsubsection{Failed Attempt}

The first way that comes to mind is this: We choose a random vector from the box $[-1, 1]^n$ where each component is chosen uniformly in $[-1, 1]$. Then we normalize this vector by dividing away its length. The resulting vector is a unit vector, so it lies on the unit sphere.

This seems like a good approach, but after some thought, we can see that the random points generated in this way aren't distributed uniformly on the sphere. Intuitively, It is more likely to choose a point near the diagonals of the box than near the center of the faces. Let's look at a two-dimensional example:

% Draw picture of square

The distance from the origin to $(1, 1)$ is $\sqrt(2)$, which is about 1.4 times the distance from the origin to the center of an edge of the square. For $n$ dimensions, the diagonal is $\sqrt{n}$ times larger. However, the odds are even worse than that. Think of two small cones of regions, one with height 1 and the other with height $\sqrt{n}$. Each cone of region will map to the same part of the sphere, but the ratio between the volumes of the cones is $(\sqrt{n})^n$. This again grows faster than exponentially.

\subsubsection{Rotational Invariance and the Multivariate Normal Distribution}

To figure out a valid way to choose a point uniformly on the sphere, we need to think outside the box. Literally. The problem with the box is that it is not rotationally invariant: If we rotate the box by a certain amount, the rotated box doesn't look the same as the original. Meanwhile, the ball and the sphere are all rotationally invariant: However we rotate them, they look the same.

Every rotation in $\R^n$ can be characterized by an orthogonal matrix. An orthogonal matrix is a matrix whose columns (or rows) form an orthonormal basis. (Note: All rotations are isometries, but not all isometries are rotations.) For instance, in $\R^2$, we can express a rotation like this:
\[
  Q = \begin{bmatrix}
    \cos(\theta) & -\sin(\theta) \\
    \sin(\theta) & \cos(\theta)
  \end{bmatrix}
\]

The columns $\xymatrix{\cos(\theta)}{\sin(\theta)}$ and $\xymatrix{-\sin(\theta)}{\cos(\theta)}$ are orthonormal because the length of both vectors are
\[
  \sqrt{\cos^2(\theta) + \sin^2(\theta)} = 1
\]
and the dot product between the two vectors is 0. Similarly, the rows of $Q$ are also orthonormal.

% Manim rotation animation

Is there any distribution that is rotationally invariant? Introducing the multivariate normal distribution. As you might know, the probability density function of a normal random variable is shaped like a bell curve. Here, we're using the standard multivariate normal distribution, denoted $\mathcal{N}(\vec{0}, I)$, which is a vector of independent random variables, where each component follows the standard normal distribution $\mathcal{N}(0, 1)$. (Here I'm using ``normal'' and ``Gaussian'' interchangeably.)

You might also know that any linear combination of independent normal random variables is still a normal random variable. We can use this fact to show that the standard multivariate normal distribution is rotationally invariant:

Suppose $Q$ is the orthogonal matrix representing an arbitrary rotation in $\R^n$, and
$X = \begin{bmatrix}
    X_1 \\ \vdots \\ X_n
  \end{bmatrix}$ is a standard multivariate normal. Then each $X_i$ is a Gaussian with mean 0 and variance 1.

Let $q_{m1}, \ldots, q_{mn}$ be the $m$th row of $Q$. Then the $m$th element of $QX$ is $Y_m = q_{m1} X_1 + \ldots + q_{mn} X_n$. Since $Q$ is orthogonal, $q_{m1}^2 + \ldots + q_{mn}^2 = 1$.

Since the $X_i$'s are independent Gaussian random variables, the linear combination is still a Gaussian random variable. Moreover, by linearity of expectation, the mean of $Y_m$ is $\mu(Y_m) = q_{m1} \mu(X_1) + \ldots + q_{mn} \mu(X_n) = 0$, and by independence, the variance of $Y_m$ is $\Var(Y_m) = q_{m1}^2 \Var(X_1) + \ldots + q_{mn}^2 \Var(X_n) = q_{m1}^2 + \ldots + q_{mn}^2 = 1$.

We have shown that $Y_m$ is a normal random variable. This is the same for each element of $QX$, so $QX$ is still a standard multivariate normal. Thus $X$ is rotationally invariant.

Here's a graph of the joint density function of a 2-dimensional standard multivariate normal random variable. We can see from the density function that the distribution is indeed rotationally invariant.

% Insert python graph here

Now we can normalize...

\subsection{Choosing the Distance}

\subsection{Python Simulation}

\section{Reflection}

\section{Acknowledgements}

\end{document}
